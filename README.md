# LLM Benchmarking Tasks

> **Terminal-Based Evaluation Framework for Large Language Models**

[![Auto-Update README](https://github.com/nbajpai-code/tb/actions/workflows/update-readme.yml/badge.svg)](https://github.com/nbajpai-code/tb/actions/workflows/update-readme.yml)

## üìä Overview

This repository contains a comprehensive set of terminal-based benchmarking tasks designed to evaluate Large Language Models (LLMs) across various capabilities:

- **Code Generation & Refinement** - Python function generation, bug fixing, and code explanation
- **Text Generation & Manipulation** - Creative writing, summarization, and paraphrasing
- **Reasoning & Logic** - Mathematical problem solving, logical deduction, and common sense reasoning
- **Information Retrieval & Q&A** - Fact retrieval and contextual question answering
- **Language Understanding** - Sentiment analysis, NER, and translation
- **Long-Context Processing** - Document summarization and complex question answering

## üìà Repository Statistics

| Metric | Value |
|--------|-------|
| **Main Task Categories** | 6 |
| **Individual Tasks** | 16 |
| **Document Size** | 25.6 KB |
| **Total Lines** | 531 |
| **Last Updated** | 2025-12-19 01:45:46 UTC |

## üìã Table of Contents

1.  [Code Generation & Refinement](#1---code-generation--refinement)
    *   [1.1 - Python Function Generation](#11---python-function-generation)
    *   [1.2 - Bug Fixing in Python](#12---bug-fixing-in-python)
    *   [1.3 - Code Explanation](#13---code-explanation)
2.  [Text Generation & Manipulation](#2---text-generation--manipulation)
    *   [2.1 - Creative Writing: Short Story](#21---creative-writing-short-story)
    *   [2.2 - Summarization of a Technical Article](#22---summarization-of-a-technical-article)
    *   [2.3 - Paraphrasing for Tone Adjustment](#23---paraphrasing-for-tone-adjustment)
3.  [Reasoning & Logic](#3---reasoning--logic)
    *   [3.1 - Mathematical Problem Solving](#31---mathematical-problem-solving)
    *   [3.2 - Logical Deduction Puzzle](#32---logical-deduction-puzzle)
    *   [3.3 - Common Sense Reasoning](#33---common-sense-reasoning)
4.  [Information Retrieval & Question Answering](#4---information-retrieval--question-answering)
    *   [4.1 - Fact Retrieval (General Knowledge)](#41---fact-retrieval-general-knowledge)
    *   [4.2 - Contextual Q&A from Provided Text](#42---contextual-qa-from-provided-text)
5.  [Language Understanding & NLU](#5---language-understanding--nlu)
    *   [5.1 - Sentiment Analysis](#51---sentiment-analysis)
    *   [5.2 - Named Entity Recognition](#52---named-entity-recognition)
    *   [5.3 - Language Translation (English to French)](#53---language-translation-english-to-french)
6.  [Long-Context Understanding](#6---long-context-understanding)
    *   [6.1 - Summarization of a Long Document](#61---summarization-of-a-long-document)
    *   [6.2 - Answering Complex Questions from Long Context](#62---answering-complex-questions-from-long-context)

---

## üöÄ Getting Started

### View the Full Documentation

The complete benchmarking tasks with detailed instructions, test cases, and evaluation criteria are available in:

**[üìÑ LLM_Benchmarking_Tasks.md](LLM_Benchmarking_Tasks.md)**

### Using the Benchmarks

Each task in the documentation includes:

1. **Objective** - What capability is being evaluated
2. **Instructions** - Clear task description for the LLM
3. **Test Cases** - Specific inputs and expected outputs
4. **Evaluation Criteria** - How to assess LLM performance
5. **Solution Strategies** - Guidance for human evaluators

### Task Categories

The benchmarks are organized into six main categories, each targeting different aspects of LLM capabilities:

1. **Code Generation & Refinement** - Evaluates programming ability
2. **Text Generation & Manipulation** - Tests creative and technical writing
3. **Reasoning & Logic** - Assesses problem-solving capabilities
4. **Information Retrieval & Q&A** - Measures knowledge access and comprehension
5. **Language Understanding & NLU** - Tests linguistic analysis skills
6. **Long-Context Understanding** - Evaluates handling of extended documents

## üéØ Use Cases

These benchmarks are designed for:

- **Model Evaluation** - Systematic assessment of LLM capabilities
- **Comparative Analysis** - Benchmarking different models against standardized tasks
- **Research & Development** - Identifying strengths and weaknesses in LLM architectures
- **Quality Assurance** - Validating model performance before deployment
- **Terminal-Based Testing** - Reproducible evaluation in command-line environments

## üìù Contributing

Contributions are welcome! If you have suggestions for additional benchmarking tasks or improvements to existing ones, please feel free to open an issue or submit a pull request.

## üìÑ License

This project is open source and available for use in LLM evaluation and research.

---

*This README is automatically updated by GitHub Actions. Last generated: 2025-12-19 01:45:46 UTC*
